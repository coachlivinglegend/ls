{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "\n",
    "from video_recorder import RecordVideo\n",
    "\n",
    "seed = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current date and time formatted as \"YYYYMMDD-HHMMSS\" to use as a unique identifier for logs\n",
    "dff = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Create a SummaryWriter object to log data for TensorBoard\n",
    "# The logs will be saved in a directory named './logs/dqn/' followed by the timestamp generated above\n",
    "writer = SummaryWriter(log_dir='./logs/dqn/'+dff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN (Deep Q-Network) model class, inheriting from PyTorch's nn.Module\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, observation_space: spaces.Box, action_space: spaces.Discrete):\n",
    "        # Initialize the parent class (nn.Module)\n",
    "        super().__init__()\n",
    "        \n",
    "        # Ensure the observation space is of type Box (continuous space)\n",
    "        assert type(observation_space) == spaces.Box, 'observation_space must be of type Box'\n",
    "        # Ensure the observation space has three dimensions: channels, width, and height\n",
    "        assert len(observation_space.shape) == 3, 'observation space must have the form channels x width x height'\n",
    "        # Ensure the action space is of type Discrete (discrete set of actions)\n",
    "        assert type(action_space) == spaces.Discrete, 'action_space must be of type Discrete'\n",
    "        \n",
    "        # Define the convolutional layers for feature extraction\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=observation_space.shape[0], out_channels=32, kernel_size=8, stride=4),  # First conv layer\n",
    "            nn.ReLU(),  # Activation function (ReLU)\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),  # Second conv layer\n",
    "            nn.ReLU(),  # Activation function (ReLU)\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),  # Third conv layer\n",
    "            nn.ReLU()  # Activation function (ReLU)\n",
    "        )\n",
    "\n",
    "        # Helper function to calculate the output size after a convolutional layer\n",
    "        def conv2d_size_out(size, kernel_size=1, stride=1, padding=0):\n",
    "            return (size - kernel_size + 2 * padding) // stride + 1\n",
    "\n",
    "        # Compute the width and height after the convolutional layers\n",
    "        convw = conv2d_size_out(observation_space.shape[2], 8, 4)  # After the first conv layer\n",
    "        convw = conv2d_size_out(convw, 4, 2)  # After the second conv layer\n",
    "        convw = conv2d_size_out(convw, 3, 1)  # After the third conv layer\n",
    "        \n",
    "        convh = conv2d_size_out(observation_space.shape[1], 8, 4)  # After the first conv layer\n",
    "        convh = conv2d_size_out(convh, 4, 2)  # After the second conv layer\n",
    "        convh = conv2d_size_out(convh, 3, 1)  # After the third conv layer\n",
    "\n",
    "        # Calculate the input size for the fully connected (fc) layers\n",
    "        linear_input_size = convw * convh * 64  # Multiply the dimensions by the number of output channels (64)\n",
    "\n",
    "        # Define the fully connected layers for action selection\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=linear_input_size, out_features=512),  # First fc layer with 512 units\n",
    "            nn.ReLU(),  # Activation function (ReLU)\n",
    "            nn.Linear(in_features=512, out_features=action_space.n)  # Output layer with units equal to the number of actions\n",
    "        )\n",
    "\n",
    "    # Define the forward pass for the network\n",
    "    def forward(self, x):\n",
    "        # Pass input through convolutional layers and flatten the output\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        # Pass the flattened output through fully connected layers and return the result\n",
    "        return self.fc(conv_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ReplayBuffer class to store and manage experience tuples for experience replay\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        # Initialize the buffer with a maximum size and tracking variables\n",
    "        self._storage = []  # List to store the experience tuples\n",
    "        self._maxsize = size  # Maximum size of the buffer\n",
    "        self._next_idx = 0  # Index to keep track of where to insert the next experience\n",
    "\n",
    "    # Return the current size of the buffer (i.e., number of stored experiences)\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    # Add a new experience tuple to the buffer\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        # Create a tuple of the experience\n",
    "        data = (state, action, reward, next_state, done)\n",
    "        # If the buffer isn't full, append the new experience\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        # If the buffer is full, overwrite the oldest experience\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        # Update the index for the next experience, wrapping around if necessary\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    # Helper function to extract and format a sample of experiences from the buffer\n",
    "    def _encode_sample(self, indices):\n",
    "        # Initialize lists to store the individual components of the experiences\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        # Loop over the provided indices and extract the corresponding experiences\n",
    "        for i in indices:\n",
    "            data = self._storage[i]\n",
    "            state, action, reward, next_state, done = data\n",
    "            # Append each component of the experience to the corresponding list\n",
    "            states.append(np.array(state, copy=False))\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(np.array(next_state, copy=False))\n",
    "            dones.append(done)\n",
    "        # Convert lists to numpy arrays and return them as a tuple\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "\n",
    "    # Sample a batch of experiences from the buffer\n",
    "    def sample(self, batch_size):\n",
    "        # Randomly select indices for the experiences to sample\n",
    "        indices = np.random.randint(0, len(self._storage) - 1, size=batch_size)\n",
    "        # Use the helper function to encode the selected experiences and return them\n",
    "        return self._encode_sample(indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to perform a random number of no-op actions at the start of an episode to add randomness\n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        # Initialize the wrapper and set the maximum number of no-op actions\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max  # Maximum number of no-op actions\n",
    "        self.override_num_noops = None  # Allows overriding the number of no-ops\n",
    "        self.noop_action = 0  # The action corresponding to 'no operation'\n",
    "        # Ensure that the first action is 'NOOP' (no operation)\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    # Reset the environment and perform a random number of no-op actions\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        # Determine the number of no-op actions to perform\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.integers(1, self.noop_max + 1)\n",
    "        assert noops > 0\n",
    "        # Perform the no-op actions\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _, _ = self.env.step(self.noop_action)\n",
    "            if done:  # If the environment is done, reset it\n",
    "                obs, info = self.env.reset(**kwargs)\n",
    "        return obs, info\n",
    "\n",
    "    # Forward the step method to the wrapped environment\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "\n",
    "\n",
    "# Wrapper that automatically performs the FIRE action at the start of an episode for environments where it is required\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        # Initialize the wrapper and ensure the environment supports the FIRE action\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    # Reset the environment and perform the FIRE action twice\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        obs, _, done, _, _ = self.env.step(1)  # Perform the FIRE action\n",
    "        if done:  # If the environment is done, reset it\n",
    "            obs, info = self.env.reset(**kwargs)\n",
    "        obs, _, done, _, _ = self.env.step(2)  # Perform the FIRE action again\n",
    "        if done:  # If the environment is done, reset it again\n",
    "            obs, info = self.env.reset(**kwargs)\n",
    "        return obs, info\n",
    "\n",
    "    # Forward the step method to the wrapped environment\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "\n",
    "\n",
    "# Wrapper that makes the environment end the episode when the agent loses a life, but not the entire game\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        # Initialize the wrapper and track the number of lives\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0  # Tracks the number of lives left\n",
    "        self.was_real_done = True  # Indicates whether the episode was really done\n",
    "\n",
    "    # Step the environment, and check if the agent lost a life\n",
    "    def step(self, action):\n",
    "        obs, reward, done, terminated, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:  # Agent lost a life but the game is not over\n",
    "            done = True  # End the episode\n",
    "        self.lives = lives  # Update the number of lives\n",
    "        return obs, reward, done, terminated, info\n",
    "\n",
    "    # Reset the environment if the episode was really done; otherwise, take a no-op action\n",
    "    def reset(self, **kwargs):\n",
    "        if self.was_real_done:\n",
    "            obs, info = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            obs, _, _, _, info = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()  # Update the number of lives\n",
    "        return obs, info\n",
    "\n",
    "\n",
    "# Wrapper that skips frames in the environment and returns the maximum frame over the skipped frames\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        # Initialize the wrapper and set the number of frames to skip\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)  # Buffer for the last two frames\n",
    "        self._skip = skip  # Number of frames to skip\n",
    "\n",
    "    # Forward the reset method to the wrapped environment\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    # Step the environment, skipping frames and returning the max frame and total reward\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        terminated = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, terminated, info = self.env.step(action)\n",
    "            if i == self._skip - 2: self._obs_buffer[0] = obs  # Store the second-to-last frame\n",
    "            if i == self._skip - 1: self._obs_buffer[1] = obs  # Store the last frame\n",
    "            total_reward += reward  # Accumulate the reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = self._obs_buffer.max(axis=0)  # Take the maximum over the buffered frames\n",
    "        return max_frame, total_reward, done, terminated, info\n",
    "\n",
    "\n",
    "# Wrapper that clips the rewards to be in the range [-1, 0, 1]\n",
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        # Initialize the wrapper\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "    # Clip the reward to the sign of the reward (-1, 0, or 1)\n",
    "    def reward(self, reward):\n",
    "        return np.sign(reward)\n",
    "\n",
    "\n",
    "# Wrapper that converts the observation frames to grayscale and resizes them\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        # Initialize the wrapper and set the target frame dimensions\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = 84  # Target width\n",
    "        self.height = 80  # Target height\n",
    "        # Update the observation space to reflect the grayscale and resized frames\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(self.height, self.width, 1), dtype=np.uint8)\n",
    "\n",
    "    # Convert the observation to grayscale and resize it\n",
    "    def observation(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)  # Convert to grayscale\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)  # Resize the frame\n",
    "        return frame[:, :, None]  # Add a channel dimension\n",
    "\n",
    "\n",
    "# Wrapper that stacks the last k frames to provide temporal information to the agent\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        # Initialize the wrapper and set up the frame stack\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k  # Number of frames to stack\n",
    "        self.frames = deque([], maxlen=k)  # Deque to store the last k frames\n",
    "        shp = env.observation_space.shape\n",
    "        # Update the observation space to reflect the stacked frames\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0] * k, shp[1], shp[2]), dtype=np.uint8)\n",
    "\n",
    "    # Reset the environment and fill the frame stack with the initial observation\n",
    "    def reset(self, **kwargs):\n",
    "        ob, info = self.env.reset(**kwargs)\n",
    "        for _ in range(self.k):  # Fill the deque with the initial observation\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob(), info\n",
    "\n",
    "    # Step the environment and update the frame stack\n",
    "    def step(self, action):\n",
    "        ob, reward, done, terminated, info = self.env.step(action)\n",
    "        self.frames.append(ob)  # Add the new observation to the frame stack\n",
    "        return self._get_ob(), reward, done, terminated, info\n",
    "\n",
    "    # Return the stacked frames as a single observation\n",
    "    def _get_ob(self):\n",
    "        assert len(self.frames) == self.k\n",
    "        return LazyFrames(list(self.frames))\n",
    "\n",
    "\n",
    "# Wrapper that scales pixel values to the range [0, 1]\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        # Initialize the wrapper\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "\n",
    "    # Scale the observation to [0, 1]\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "# Utility class to handle lazy loading of frames, avoiding unnecessary memory usage\n",
    "class LazyFrames(object):\n",
    "    def __init__(self, frames):\n",
    "        # Store the frames\n",
    "        self._frames = frames\n",
    "\n",
    "    # Convert the frames to a single numpy array when needed\n",
    "    def __array__(self, dtype=None):\n",
    "        out = np.concatenate(self._frames, axis=0)  # Concatenate frames along the first axis\n",
    "        if dtype is not None:\n",
    "            out = out.astype(dtype)  # Cast to the desired dtype if provided\n",
    "        return out\n",
    "\n",
    "    # Return the number of frames\n",
    "    def __len__(self):\n",
    "        return len(self._frames)\n",
    "\n",
    "    # Return a specific frame by index\n",
    "    def __getitem__(self, i):\n",
    "        return self._frames[i]\n",
    "\n",
    "\n",
    "# Wrapper that changes the observation format to be compatible with PyTorch (channel-first)\n",
    "class PyTorchFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(PyTorchFrame, self).__init__(env)\n",
    "        shape = self.observation_space.shape\n",
    "        # Update the observation space to be channel-first (C x H x W)\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(shape[-1], shape[0], shape[1]), dtype=np.uint8)\n",
    "\n",
    "    # Roll the channel axis to the first position\n",
    "    def observation(self, observation):\n",
    "        return np.rollaxis(observation, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQNAgent class that implements the DQN algorithm\n",
    "class DQNAgent:\n",
    "    def __init__(self, observation_space: spaces.Box, action_space: spaces.Discrete, replay_buffer: ReplayBuffer, lr, batch_size, gamma):\n",
    "        # Initialize the replay buffer, batch size, and discount factor\n",
    "        self.memory = replay_buffer\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Set the device to GPU if available, otherwise fallback to CPU\n",
    "        device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() else\n",
    "        \"mps\" if torch.backends.mps.is_available() else\n",
    "        \"cpu\")\n",
    "        print(f\"Using device {device}\")\n",
    "\n",
    "        # Initialize the policy network and target network\n",
    "        self.policy_network = DQN(observation_space, action_space).to(device)\n",
    "        self.target_network = DQN(observation_space, action_space).to(device)\n",
    "        self.update_target_network()  # Copy weights from policy network to target network\n",
    "        self.target_network.eval()  # Set the target network to evaluation mode\n",
    "\n",
    "        # Initialize the optimizer for the policy network using RMSprop\n",
    "        self.optimiser = torch.optim.RMSprop(self.policy_network.parameters(), lr=lr)\n",
    "        \n",
    "        # Store the device (CPU, CUDA, or MPS) for later use\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "    # Function to optimize the temporal difference loss\n",
    "    def optimise_td_loss(self):\n",
    "        device = self.device\n",
    "        # Sample a batch of experiences from the replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        # Normalize states and next_states to be in the range [0, 1]\n",
    "        states = np.array(states) / 255.0\n",
    "        next_states = np.array(next_states) / 255.0\n",
    "        \n",
    "        # Convert the arrays to PyTorch tensors and move them to the selected device\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        actions = torch.from_numpy(actions).long().to(device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(device)\n",
    "        dones = torch.from_numpy(dones).float().to(device)\n",
    "\n",
    "        # Calculate the target Q-values using the target network\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states)\n",
    "            max_next_q_values, _ = next_q_values.max(1)\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * max_next_q_values\n",
    "\n",
    "        # Get the Q-values from the policy network for the selected actions\n",
    "        input_q_values = self.policy_network(states)        \n",
    "        input_q_values = input_q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "\n",
    "        # Calculate the loss using the smooth L1 loss (Huber loss)\n",
    "        loss = F.smooth_l1_loss(input_q_values, target_q_values)\n",
    "\n",
    "        # Perform backpropagation and update the policy network's weights\n",
    "        self.optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimiser.step()\n",
    "\n",
    "        # Clear the variables to free memory\n",
    "        del states\n",
    "        del next_states\n",
    "        \n",
    "        # Return the loss value for logging\n",
    "        return loss.item()\n",
    "\n",
    "    # Function to copy the weights from the policy network to the target network\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "\n",
    "    # Function to select an action using the policy network\n",
    "    def act(self, state: np.ndarray):\n",
    "        device = self.device\n",
    "        # Normalize the state and convert it to a PyTorch tensor\n",
    "        state = np.array(state) / 255.0\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        # Use the policy network to get Q-values and select the action with the highest Q-value\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_network(state)\n",
    "            _, action = q_values.max(1)\n",
    "            return action.item()\n",
    "\n",
    "    # Function to save the model's state to a file\n",
    "    def save(self, path):\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        \n",
    "        # Save the policy network, target network, and optimizer states\n",
    "        torch.save({\n",
    "            'policy_network_state_dict': self.policy_network.state_dict(),\n",
    "            'target_network_state_dict': self.target_network.state_dict(),\n",
    "            'optimizer_state_dict': self.optimiser.state_dict(),\n",
    "        }, path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "\n",
    "    # Function to load the model's state from a file\n",
    "    def load(self, path):\n",
    "        # Load the saved state dictionaries and restore them to the networks and optimizer\n",
    "        checkpoint = torch.load(path)\n",
    "        self.policy_network.load_state_dict(checkpoint['policy_network_state_dict'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])\n",
    "        self.optimiser.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(f\"Model loaded from {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Hyperparameters and Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owner\\.conda\\envs\\all4gpu\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f35a18f160>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a dictionary to hold hyperparameters for the DQN training process\n",
    "hyper_params = {\n",
    "    \"env\": \"TennisDeterministic-v4\",  # The environment to be used (a specific variant of the Tennis game)\n",
    "    \"replay-buffer-size\": int(5e3),  # Maximum size of the replay buffer (5000 experiences)\n",
    "    \"learning-rate\": 1e-4,  # Learning rate for the optimizer\n",
    "    \"discount-factor\": 0.95,  # Discount factor (gamma) for future rewards\n",
    "    \"num-steps\": int(1e6),  # Total number of steps to run the training for (1,000,000 steps)\n",
    "    \"batch-size\": 32,  # Size of the mini-batch sampled from the replay buffer\n",
    "    \"learning-starts\": 10000,  # Number of steps before learning starts (to populate replay buffer)\n",
    "    \"learning-freq\": 1,  # Frequency of learning updates (every 1 step)\n",
    "    \"target-update-freq\": 100,  # Frequency of updating the target network (every 100 steps)\n",
    "    \"eps-start\": 1,  # Starting value of epsilon for epsilon-greedy exploration\n",
    "    \"eps-end\": 0.05,  # Minimum value of epsilon for epsilon-greedy exploration\n",
    "    \"eps-decay\": 0.995,  # Rate at which epsilon decays after each episode\n",
    "    \"print-freq\": 10  # Frequency of printing updates during training (every 10 episodes)\n",
    "}\n",
    "\n",
    "# Create the environment using the specified environment name from hyperparameters\n",
    "env = gym.make(hyper_params[\"env\"], render_mode='rgb_array')\n",
    "env.reset()  # Reset the environment to its initial state\n",
    "plt.imshow(env.render())  # Render the environment and display the initial state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "# Apply a series of environment wrappers to preprocess observations and actions\n",
    "env = NoopResetEnv(env, noop_max=30)  # Apply random no-ops at the start of an episode for added randomness\n",
    "env = MaxAndSkipEnv(env, skip=4)  # Skip frames and return the max frame over the skipped frames for efficiency\n",
    "env = EpisodicLifeEnv(env)  # Make episodes end when a life is lost, but not the entire game\n",
    "env = FireResetEnv(env)  # Automatically perform the FIRE action at the start of episodes where required\n",
    "env = WarpFrame(env)  # Convert frames to grayscale and resize them to 84x80 for input to the network\n",
    "env = PyTorchFrame(env)  # Reorder frame dimensions to be channel-first for compatibility with PyTorch\n",
    "env = ClipRewardEnv(env)  # Clip rewards to be in the range [-1, 0, 1] for stability\n",
    "env = FrameStack(env, 4)  # Stack the last 4 frames together to provide temporal context to the agent\n",
    "\n",
    "# Record the environment's video every 200 episodes and save it to the './video/' directory\n",
    "env = RecordVideo(env, './video/', episode_trigger=lambda episode_id: episode_id % 200 == 0)\n",
    "\n",
    "# Create a replay buffer with the specified size from hyperparameters\n",
    "replay_buffer = ReplayBuffer(hyper_params[\"replay-buffer-size\"])\n",
    "\n",
    "# Initialize the DQN agent with the environment's observation and action spaces,\n",
    "# the replay buffer, learning rate, batch size, and discount factor from the hyperparameters\n",
    "agent = DQNAgent(\n",
    "    env.observation_space,  # Observation space of the environment\n",
    "    env.action_space,  # Action space of the environment\n",
    "    replay_buffer,  # Replay buffer for storing experiences\n",
    "    lr=hyper_params['learning-rate'],  # Learning rate for the optimizer\n",
    "    batch_size=hyper_params['batch-size'],  # Batch size for training\n",
    "    gamma=hyper_params['discount-factor'],  # Discount factor for future rewards\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video c:\\Users\\Owner\\Desktop\\AIML\\L2\\ReinforcementLearningProgramming\\Project\\video\\rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video c:\\Users\\Owner\\Desktop\\AIML\\L2\\ReinforcementLearningProgramming\\Project\\video\\rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\Owner\\Desktop\\AIML\\L2\\ReinforcementLearningProgramming\\Project\\video\\rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************\n",
      "steps: 3388\n",
      "episodes: 10\n",
      "mean 100 episode reward: -24.0\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 17303\n",
      "episodes: 20\n",
      "mean 100 episode reward: -23.9\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 34688\n",
      "episodes: 30\n",
      "mean 100 episode reward: -23.9\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 46427\n",
      "episodes: 40\n",
      "mean 100 episode reward: -23.9\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 59829\n",
      "episodes: 50\n",
      "mean 100 episode reward: -23.9\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 68235\n",
      "episodes: 60\n",
      "mean 100 episode reward: -23.8\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 75295\n",
      "episodes: 70\n",
      "mean 100 episode reward: -23.7\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 81724\n",
      "episodes: 80\n",
      "mean 100 episode reward: -23.7\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 87284\n",
      "episodes: 90\n",
      "mean 100 episode reward: -23.6\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 92964\n",
      "episodes: 100\n",
      "mean 100 episode reward: -23.6\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 99741\n",
      "episodes: 110\n",
      "mean 100 episode reward: -23.5\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 108242\n",
      "episodes: 120\n",
      "mean 100 episode reward: -23.4\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 114647\n",
      "episodes: 130\n",
      "mean 100 episode reward: -23.3\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 122338\n",
      "episodes: 140\n",
      "mean 100 episode reward: -23.1\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 130053\n",
      "episodes: 150\n",
      "mean 100 episode reward: -22.9\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 137332\n",
      "episodes: 160\n",
      "mean 100 episode reward: -22.7\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 144899\n",
      "episodes: 170\n",
      "mean 100 episode reward: -22.4\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 153472\n",
      "episodes: 180\n",
      "mean 100 episode reward: -22.0\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 160964\n",
      "episodes: 190\n",
      "mean 100 episode reward: -21.7\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 169599\n",
      "episodes: 200\n",
      "mean 100 episode reward: -21.3\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "Moviepy - Building video c:\\Users\\Owner\\Desktop\\AIML\\L2\\ReinforcementLearningProgramming\\Project\\video\\rl-video-episode-200.mp4.\n",
      "Moviepy - Writing video c:\\Users\\Owner\\Desktop\\AIML\\L2\\ReinforcementLearningProgramming\\Project\\video\\rl-video-episode-200.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\Owner\\Desktop\\AIML\\L2\\ReinforcementLearningProgramming\\Project\\video\\rl-video-episode-200.mp4\n",
      "********************************************************\n",
      "steps: 177336\n",
      "episodes: 210\n",
      "mean 100 episode reward: -20.9\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 186015\n",
      "episodes: 220\n",
      "mean 100 episode reward: -20.5\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 195779\n",
      "episodes: 230\n",
      "mean 100 episode reward: -20.1\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 203553\n",
      "episodes: 240\n",
      "mean 100 episode reward: -19.8\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 212181\n",
      "episodes: 250\n",
      "mean 100 episode reward: -19.5\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 220012\n",
      "episodes: 260\n",
      "mean 100 episode reward: -19.3\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 228626\n",
      "episodes: 270\n",
      "mean 100 episode reward: -19.0\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 237603\n",
      "episodes: 280\n",
      "mean 100 episode reward: -18.8\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 246072\n",
      "episodes: 290\n",
      "mean 100 episode reward: -18.6\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 255299\n",
      "episodes: 300\n",
      "mean 100 episode reward: -18.6\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 264560\n",
      "episodes: 310\n",
      "mean 100 episode reward: -18.2\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 272856\n",
      "episodes: 320\n",
      "mean 100 episode reward: -18.0\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 281636\n",
      "episodes: 330\n",
      "mean 100 episode reward: -17.8\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 290912\n",
      "episodes: 340\n",
      "mean 100 episode reward: -17.7\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 299616\n",
      "episodes: 350\n",
      "mean 100 episode reward: -17.4\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "Model saved to model_20240721-163231/checkpoint_350.pth\n",
      "********************************************************\n",
      "steps: 308895\n",
      "episodes: 360\n",
      "mean 100 episode reward: -17.1\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 318735\n",
      "episodes: 370\n",
      "mean 100 episode reward: -17.0\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 329264\n",
      "episodes: 380\n",
      "mean 100 episode reward: -16.8\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 338450\n",
      "episodes: 390\n",
      "mean 100 episode reward: -16.4\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "********************************************************\n",
      "steps: 349517\n",
      "episodes: 400\n",
      "mean 100 episode reward: -16.1\n",
      "% time spent exploring: 5\n",
      "********************************************************\n",
      "Model saved to model_20240721-163231/checkpoint_400.pth\n",
      "Moviepy - Building video c:\\Users\\Owner\\Desktop\\AIML\\L2\\ReinforcementLearningProgramming\\Project\\video\\rl-video-episode-400.mp4.\n",
      "Moviepy - Writing video c:\\Users\\Owner\\Desktop\\AIML\\L2\\ReinforcementLearningProgramming\\Project\\video\\rl-video-episode-400.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\Owner\\Desktop\\AIML\\L2\\ReinforcementLearningProgramming\\Project\\video\\rl-video-episode-400.mp4\n",
      "********************************************************\n",
      "steps: 359666\n",
      "episodes: 410\n",
      "mean 100 episode reward: -16.0\n",
      "% time spent exploring: 5\n",
      "********************************************************\n"
     ]
    }
   ],
   "source": [
    "# Initialize the epsilon threshold for epsilon-greedy exploration\n",
    "eps_threshold = hyper_params[\"eps-start\"]\n",
    "\n",
    "# List to store rewards for each episode\n",
    "episode_rewards = [0.0]\n",
    "\n",
    "# List to store cumulative rewards for logging\n",
    "cumulative_rewards = []\n",
    "\n",
    "# Reset the environment and get the initial state\n",
    "state, _ = env.reset(seed=seed)\n",
    "\n",
    "# Main training loop running for the specified number of steps\n",
    "for t in range(hyper_params[\"num-steps\"]):\n",
    "        \n",
    "    # Update epsilon for exploration-exploitation trade-off\n",
    "    if eps_threshold > hyper_params[\"eps-end\"]:\n",
    "        eps_threshold *= hyper_params[\"eps-decay\"]\n",
    "    else:\n",
    "        eps_threshold = hyper_params[\"eps-end\"]\n",
    "    \n",
    "    # Decide whether to explore or exploit based on epsilon-greedy strategy\n",
    "    sample = random.random()\n",
    "    if sample > eps_threshold:\n",
    "        action = agent.act(state)  # Exploit: choose the best action according to the policy\n",
    "    else:\n",
    "        action = env.action_space.sample()  # Explore: choose a random action\n",
    "\n",
    "    # Take the action and observe the next state, reward, and done flag\n",
    "    next_state, reward, done, info, _ = env.step(action)\n",
    "    \n",
    "    # Add the experience to the replay buffer\n",
    "    agent.memory.add(state, action, reward, next_state, float(done))\n",
    "    \n",
    "    # Update the current state\n",
    "    state = next_state\n",
    "\n",
    "    # Accumulate the reward for the current episode\n",
    "    episode_rewards[-1] += reward\n",
    "    \n",
    "    # If the episode is done, reset the environment and start a new episode\n",
    "    if done:\n",
    "        state, _ = env.reset(seed=seed)\n",
    "        episode_rewards.append(0.0)\n",
    "\n",
    "    # Start training after a certain number of steps and update the policy network\n",
    "    if t > hyper_params[\"learning-starts\"] and t % hyper_params[\"learning-freq\"] == 0:\n",
    "        loss = agent.optimise_td_loss()\n",
    "        writer.add_scalar('Loss', loss, t)  # Log loss to TensorBoard\n",
    "\n",
    "    # Periodically update the target network to match the policy network\n",
    "    if t > hyper_params[\"learning-starts\"] and t % hyper_params[\"target-update-freq\"] == 0:\n",
    "        agent.update_target_network()\n",
    "\n",
    "    # Track the number of completed episodes\n",
    "    num_episodes = len(episode_rewards)\n",
    "\n",
    "    # If the episode is done, calculate and log cumulative rewards\n",
    "    if done:\n",
    "        cumulative_reward = sum(episode_rewards[-5:]) / 5  # Average reward over the last 5 episodes\n",
    "        cumulative_rewards.append(cumulative_reward)\n",
    "\n",
    "        # Log various metrics to TensorBoard\n",
    "        writer.add_scalar('Score per Step', episode_rewards[-2], t)  # Log score at each step\n",
    "        writer.add_scalar('Score per Episode', episode_rewards[-2], num_episodes)  # Log score per episode\n",
    "        writer.add_scalar('Average Cumulative Reward (Last 5 Episodes)', cumulative_reward, num_episodes)  # Log avg cumulative reward\n",
    "\n",
    "        # Print progress every few episodes\n",
    "        if hyper_params[\"print-freq\"] is not None and len(episode_rewards) % hyper_params[\"print-freq\"] == 0:\n",
    "            mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)\n",
    "            print(\"********************************************************\")\n",
    "            print(\"steps: {}\".format(t))\n",
    "            print(\"episodes: {}\".format(num_episodes))\n",
    "            print(\"mean 100 episode reward: {}\".format(mean_100ep_reward))\n",
    "            print(\"% time spent exploring: {}\".format(int(100 * eps_threshold)))\n",
    "            print(\"********************************************************\")\n",
    "            \n",
    "            # Save a checkpoint of the policy network and rewards\n",
    "            torch.save(agent.policy_network.state_dict(), f'checkpoint.pth')\n",
    "            np.savetxt('rewards_per_episode.csv', episode_rewards, delimiter=',', fmt='%1.3f')\n",
    "            np.savetxt('cumulative_rewards.csv', cumulative_rewards, delimiter=',', fmt='%1.3f')\n",
    "        \n",
    "        # Periodically save the model after a certain number of episodes\n",
    "        if num_episodes > 300 and num_episodes % 50 == 0:\n",
    "            agent.save(f'model_{dff}/checkpoint_{num_episodes}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final trained model to a file at the end of training\n",
    "agent.save(f'model_{dff}/final_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rewards\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward per Episode')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all4gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
